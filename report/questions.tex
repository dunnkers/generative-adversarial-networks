% <BNAIC-STYLE>
\documentclass{article}
\usepackage{bnaic}
\usepackage{todonotes}
\usepackage{svg}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{array}
\usepackage{wrapfig}
\usepackage{tabularx}

\title{\textbf{\huge Making Art with Generative Adversarial Networks - Answers to Questions}}
\author{Loran Knol (s3182541)\\Thijs Havinga (s2922924)\\Elisa Oostwal (s2468255)\\Jeroen Overschie (s2995697)}
\date{\textit{University of Groningen, Nijenborgh 9 9747AG Groningen}}

\pagestyle{empty}

% * Which software did you make use of? Which code did you use that you found on Internet (send links). What did you change to the code? Where did you download the datasets from? Has any hyper-parameter tuning been done? Which computers did you use (your own PC with GPU/CPU, Peregrine)? How long did the experiments on average take for a single run?

% * What are the contributions of each group member? Who coded what? Who did which experiments? Who created the figures / tables with results? Who wrote which parts of the paper? 

\begin{document}
\ttl
\thispagestyle{empty}

\section{Implementation}

\subsection{Which software did you make use of?}
For StyleGAN, we used TensorFlow 1.10 with CUDA 9.0, running on Python 3.6. Of course we also used the code for StyleGAN itself (link is given in Question \ref{_sec:internet_code}).

\subsection{Which code did you use that you found on Internet (send links). What did you change to the code?}\label{_sec:internet_code}
StyleGAN repository: \url{https://github.com/NVlabs/stylegan}. Changes were mainly made to \verb|train.py|, which configured the training schedule (progressive growing vs. constant size).

\subsection{Where did you download the datasets from?}
The Van Gogh dataset: \url{https://www.kaggle.com/ipythonx/van-gogh-paintings}.

\subsection{Has any hyper-parameter tuning been done?}
Since training the StyleGAN instances was a very costly process, we did not include too many variations of the network in our experiment. We mainly changed whether the training schedule was reset before training or kept in the same state as it was for the original StyleGAN instance. This influenced the minibatch size, the learning rate and the resolution StyleGAN had to work with.

\subsection{Which computers did you use (your own PC with GPU/CPU, Peregrine)?}
We used a Peregrine node with two NVIDIA Tesla K40 GPUs for training the StyleGAN instances. The Fr√©chet Inception distances (FIDs) were calculated on one of our own laptops that had an RTX 2060 GPU. Some early experiments were ran in Kaggle's Notebook environment, running on NVidia K80 GPUs.

\subsection{How long did the experiments on average take for a single run?}
Every training run (progressively grown or constant-sized network) took 24 hours. We believe more training would have been beneficial for the results, but after a bunch of testing runs we had to start wrapping things up. Calculating the FIDs took 4-5 hours for every trained network.

\section{Division of work}

\subsection{What are the contributions of each group member?}
% Deze tabel is een stub, voeg vooral dingen toen
\begin{table}[h!]
    \centering
    \begin{tabularx}{\linewidth}{l|X|X|X|X}
                        & Loran Knol                                & Thijs Havinga                     & Elisa Oostwal                 & Jeroen Overschie  \\
        \hline
        Coded           & StyleGAN                                  & DCGAN and data                    & Plug and Play                 & TensorFlow.js     \\
        \hline
        Experiments     & Constant-sized StyleGAN                   & DCGAN                             & Progressively-grown StyleGAN  &  Notebook experiments with DCGAN, Progressive GAN   \\
        \hline
        Figures/tables  & All figures related to StyleGAN results   & All figures related to DCGAN                                  &                               &                   \\
        \hline
        Paper parts     & StyleGAN methods and results              & Data augmentation, DCGAN methods and results   & Data, Conclusion                    & Introduction and GAN background             
    \end{tabularx}
    % \caption{Caption}
    \label{tab:my_label}
\end{table}

\end{document}